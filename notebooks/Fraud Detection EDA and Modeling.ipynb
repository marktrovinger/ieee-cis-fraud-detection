{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "%matplotlib inline\n",
    "\n",
    "# disable the warning about settingwithcopy warning:\n",
    "pd.set_option('chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory_path = \"/kaggle/input/ieee-fraud-detection/\"\n",
    "os.chdir(working_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 25.86 Mb (42.7% reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9e32179ac91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_identity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_identity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_transaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_transaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_identity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_identity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_transaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_transaction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7b7e1b1ba1c9>\u001b[0m in \u001b[0;36mreduce_mem_usage\u001b[0;34m(df, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mc_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_max\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mc_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_max\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3467\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3544\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3545\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3547\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value)\u001b[0m\n\u001b[1;32m   1097\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                     \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4422\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4423\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4424\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_identity = pd.read_csv(\"train_identity.csv\")\n",
    "train_transaction = pd.read_csv(\"train_transaction.csv\")\n",
    "\n",
    "test_identity = pd.read_csv(\"test_identity.csv\")\n",
    "test_transaction = pd.read_csv(\"test_transaction.csv\")\n",
    "\n",
    "train_identity = reduce_mem_usage(train_identity)\n",
    "train_transaction = reduce_mem_usage(train_transaction)\n",
    "test_identity = reduce_mem_usage(test_identity)\n",
    "test_transaction = reduce_mem_usage(test_transaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "\n",
    "**Predict fraud based on transaction information**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the number of columns are too large, we can expand it using pd.set_option()\n",
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining the transaction data:\n",
    "\n",
    "### Transaction Table:\n",
    "\n",
    "Transaction table contain TransactionID, which we can use to join with the identity table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transaction['TransactionID'].value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction ID for transaction table is also unique for each observation. Therefore, we have 1 to 1 join from identity table to transaction table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.merge(train_identity, train_transaction, on = 'TransactionID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it comes to my attention that the number of rows are different for each table, despite having unique TransactionID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of row in transaction:', len(train_transaction))\n",
    "print('Number of row in identity:', len(train_identity))\n",
    "\n",
    "# remove train_transaction from memory\n",
    "# del train_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that there are transactions that don't have identity. A quick research on the [discussion thread](https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203#latest-605862) reveals that Vesta was unable to collect all identity information due to technical difficulty. Therefore, we will need to face two options:\n",
    "\n",
    "1. Using identity + transaction to make predictions. This option results in fewer observations but more complete (more features).\n",
    "\n",
    "2. Using only transaction\n",
    "\n",
    "3. Using transaction but add identity when avaiable\n",
    "\n",
    "For now, we only explore the identity + transaction joined table to do EDA and build model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Inspection\n",
    "\n",
    "There are couple common issues that we need to watch out for:\n",
    "\n",
    "1. Attributes Formatting (data types)\n",
    "\n",
    "2. Missing Data\n",
    "\n",
    "3. Replacement or Drop\n",
    "\n",
    "4. Response Variable\n",
    "\n",
    "First, let's transform our data into the types that we expected:\n",
    "\n",
    "## 1. Attributes formatting (data types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Most of the column names have been masked for privacy protection. Without accurate description of the fields meaning, it would be difficult to determine the type of data. Fortunately, Vesta have provided us with high-level summary of data.\n",
    "\n",
    "Let's recall the avaiable groups of information that were provided for us:\n",
    "\n",
    "1. Identity Table:\n",
    "\n",
    "    * id_01 - id_38: contains network connection information\n",
    "    \n",
    "    * DeviceType and DeviceInfo\n",
    "    \n",
    "2. Transactional Table:\n",
    "\n",
    "    * card1 - card6: card information\n",
    "    \n",
    "    * addr: address\n",
    "    \n",
    "    * dist: distance\n",
    "\n",
    "    * P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "    \n",
    "    * C1-C14: counting\n",
    "    \n",
    "    * D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "    \n",
    "    * M1-M9: match, such as names on card and address, etc.\n",
    "    \n",
    "    * V1-V339: Vesta engineered features\n",
    "    \n",
    "    * ProductCD: product code, the product for each transaction\n",
    "    \n",
    "    * TransactionDT: timedelta from a given reference datetime\n",
    "    \n",
    "    * TransactionAMT: transaction payment amount in USD    \n",
    "    \n",
    "## Missing Data\n",
    "    \n",
    "Let's take a look at the missing data for the **categorical variables** first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat = train_full.filter(regex='id|card|ProductCD|addr|email|M|DeviceType|DeviceInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,9))\n",
    "sns.heatmap(train_full_cat.isnull(), cbar= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: We can see that our data has a lot of missing values. White color presents missing values.\n",
    "\n",
    "1. Most M columns missing almost if not all data\n",
    "\n",
    "2. Id_07, 08 and id_21-27 missing most data\n",
    "\n",
    "3. Id_01, id_12, card1, card2 contains mostly non-null. Perhaps, these columns contain unique ID information, and therefore, cannot be null. Let's double check the number of missing in these columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat[['id_01','id_12','card1','card2']].info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are indeed complete, except for card 2. If I were to guess, card1 could be first name and card2 could be last name.\n",
    "\n",
    "Now let's check out missing data for **numerical variables**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_num = train_full.filter(regex='isFraud|TransactionDT|TransactionAmt|dist|C|D')\n",
    "plt.figure(figsize=(18,9))\n",
    "sns.heatmap(train_full_num.isnull(), cbar= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "    \n",
    "    1. Basic information about transaction such as ID, DT, amount and type of product is complete \n",
    "    \n",
    "    2. Dist1 and dist2 is very sparse.\n",
    "    \n",
    "    3. C columns are complete\n",
    "    \n",
    "    4. Most D columns are sparse except D1\n",
    "    \n",
    "Lastly, we want to check for data completeness of **Vesta's engineered features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_Vesta = train_full.filter(regex='V')\n",
    "plt.figure(figsize=(18,9))\n",
    "sns.heatmap(train_full_Vesta.isnull(), cbar= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Ahh, she looks like a work of art. The repeated missing patterns in the V columns suggest that many V columns are related and perhaps trying to describe certain characteristics of a transaction. For example, columns V322-V399 have identical missing locations.\n",
    "\n",
    "Let's verify our intuition with correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "msno.dendrogram(train_full_Vesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: The dendrogram uses a hierarchical clustering algorithm to bin variables against one another by their nullity correlation. Each cluster of leaves explain how one variable might always be empty when another is empty, or filled when another variable is filled. This dendogram suggests that the position of missing/fill values are correlated. Perhaps similar columns were derived from the same feature or combinations of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Replacement or drop the missings\n",
    "The idea of imputation is both *\"seductive and dangerous\"* in the words of R.J.A Little. \n",
    "\n",
    "I truly believe that there is no best way to deal with missing, especially when having to deal with partial information. Knowing which columns could be imputed or dropped may alter the result of the final predictions by a non-trivial amount. The fact that certain value is missing could have been due to specific variation in the feature (missing not at random). This is one of the process that could have been much more useful if we were given the meaning of each columns. But when life gives you lemon, you turns it into sweet, sweet meachine learning input juice. \n",
    "\n",
    "### Understand that Train and Test data were splitted by time\n",
    "\n",
    "This is a graceful finding from https://www.kaggle.com/robikscube/ieee-fraud-detection-first-look-and-eda.\n",
    "\n",
    "* The `TransactionDT` feature is defined as time delta from a chosen datetime. This gives us information about the relative time and the countinuity of each transaction. Ploting both test and train `TransactionDT` on the graph suggests that train and test dataset were splited by time, with a gap in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_transaction['TransactionDT'], label='train')\n",
    "plt.hist(test_transaction['TransactionDT'], label='test')\n",
    "plt.legend()\n",
    "plt.title('Distribution of TransactionDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some people suggest that if `TransactionDT` is measured in seconds, then the combined time period between test and train dataset could total to approximately 1 year, and the gap can account for ~ 1 month. \n",
    "\n",
    "Lynn@Vesta commented in one of the discussion post:\n",
    "\n",
    "*\"We define reported chargeback on card, user account, associated email address and transactions directly linked to these attributes as fraud transaction (isFraud=1); If none of above is found after 120 days, then we define as legit (isFraud=0)\"*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response/ Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "g = sns.countplot(x = 'isFraud', data = train_full)\n",
    "g.set_title(\"Fraud Distribution\", fontsize = 17)\n",
    "g.set_xlabel(\"Is Fraud?\", fontsize = 15)\n",
    "g.set_ylabel(\"Count\", fontsize = 15)\n",
    "plt.legend(title='Fraud', labels=['No', 'Yes'])\n",
    "\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(train_full) * 100),\n",
    "            ha=\"center\", fontsize=15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:The fraud percentage is quite high: 7.85% for the complete observations (identity + transaction). We can see there is a class imbalance problem, where occurence of one class is significantly higher than another. This will lead to much a higher false negative - tendency of picking \"not fraud\". We can mitigate this issue by using two common methods:\n",
    "\n",
    "1. Cost function based approaches\n",
    "\n",
    "2. Sampling based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Categorical Features\n",
    "**Categorical Features:**\n",
    "\n",
    "**1. Transactional Table:**\n",
    "    \n",
    "    ProductCD\n",
    "\n",
    "    card1 - card6\n",
    "\n",
    "    addr1, addr2\n",
    "\n",
    "    Pemaildomain Remaildomain\n",
    "\n",
    "    M1 - M9\n",
    "    \n",
    "    \n",
    "**2. Identity Table**\n",
    "\n",
    "    DeviceType\n",
    "\n",
    "    DeviceInfo\n",
    "    \n",
    "    id12 - id38\n",
    "\n",
    "Let's take a quick look at these categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Product Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "total = len(train_full_cat)\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.countplot(x = 'ProductCD', data = train_full_cat)\n",
    "g.set_title('ProductCD Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Product Code\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=14) \n",
    "\n",
    "plt.subplot(122)\n",
    "g1 = sns.countplot(x='ProductCD', hue='isFraud', data=train_full)\n",
    "g1.set_title('ProductCD by Fraud', fontsize = 15)\n",
    "g1.set_xlabel(\"Product Code\", fontsize=15)\n",
    "g1.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Observations**: C is the most frequent product category. Product C also have the highest count of fraud. We can obtain the proportion of fraud for each product category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full['isFraud'] == 1]['ProductCD'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped table\n",
    "train_full.groupby('ProductCD')['isFraud'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of table\n",
    "plt.figure(figsize=(12,12))\n",
    "a = train_full.groupby('ProductCD')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\n",
    "a.set_title('Rate of Fraud by Product Category', fontsize = 15)\n",
    "plt.xticks(rotation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Product C takes up 67.5% of fraud cases for transactions that have identity. And also have highest rate of fraud: 12%, more than double any other class of product.\n",
    "\n",
    "**Question**: Why product C? Is there any additional information that help us better understand product C high fraud rate?\n",
    "\n",
    "We have 2 numerical variables that we can compare between groups of products:\n",
    "\n",
    "TransactionDT: timedelta from a given reference datetime\n",
    "\n",
    "TransactionAmt: transaction payment amount in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(x = 'ProductCD', y = 'TransactionAmt', hue = 'isFraud', data = train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Product C are items with low dollar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(x = 'ProductCD', y = 'TransactionDT', hue = 'isFraud', data = train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: All products have same min and max timedelta range. \n",
    "\n",
    "**Conclusion**:The plot suggests little to no difference in timedelta accross all groups.\n",
    "\n",
    "## Examine Card 1,2,3,5\n",
    "\n",
    "The card 1,2,3, and 5 was represented as numerical values, temping us to plot the histogram. However, we need to remember that card columns were classified as categorical variables. Meaning it's likely that these numerical variables were coded for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat.describe().loc[:,'card1':'card5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat.loc[:,'card1':'card5'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Card 1 contains 8499 unique values, suggesting card 1 may have been ID of the card. Card 2,3 and 5 have less unique values, so perhaps they could be expiration date, or combinations that generate card identity? Since we don't know how these information was scrammbled, we might pickup patterns generated by encryption algorithm instead of data. No further analysis should be done unless more infomation is given.\n",
    "\n",
    "Same goes for the addr1 and addr2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Card 4 and 6\n",
    "\n",
    "### Card 4: Card Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "total = len(train_full_cat)\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.countplot(x = 'card4', data = train_full_cat)\n",
    "g.set_title('Card Network Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Card Issuers\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=14) \n",
    "\n",
    "plt.subplot(122)\n",
    "g1 = sns.countplot(x='card4', hue='isFraud', data=train_full)\n",
    "g1.set_title('Card Network by Fraud', fontsize = 15)\n",
    "g1.set_xlabel(\"Card Issuers\", fontsize=15)\n",
    "g1.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Visa card accounts for the highest instances of fraud, but this also because visa is the most popular card type. Again, we can only conclude after comparing the fraud propotion for each card type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[train_full['isFraud'] == 1]['card4'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped table\n",
    "train_full.groupby('card4')['isFraud'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of table\n",
    "plt.figure(figsize=(12,12))\n",
    "b = train_full.groupby('card4')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\n",
    "b.set_title('Rate of Fraud by Card Network', fontsize = 15)\n",
    "plt.xticks(rotation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Visa accounts for 61% of all fraud occurences. However, when normalized by total number of each type, Visa have fraud rate of only 8%, lower than Mastercard and same as Discovercard. Only American Express have significantly lower fraud rate compare to others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card 6: Card Type\n",
    "\n",
    "Similarly, we can use the same method of data analysis on this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "total = len(train_full_cat)\n",
    "\n",
    "plt.subplot(121)\n",
    "g = sns.countplot(x = 'card6', data = train_full)\n",
    "g.set_title('Card Type Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Card Type\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "for p in g.patches:\n",
    "    height = p.get_height()\n",
    "    g.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=14) \n",
    "\n",
    "plt.subplot(122)\n",
    "g1 = sns.countplot(x='card6', hue='isFraud', data=train_full)\n",
    "g1.set_title('Card Type by Fraud', fontsize = 15)\n",
    "g1.set_xlabel(\"Card Type\", fontsize=15)\n",
    "g1.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The number of card type are fairly simiar, and so does the fraud cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped table\n",
    "train_full.groupby('card6')['isFraud'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of table\n",
    "plt.figure(figsize=(12,12))\n",
    "b = train_full.groupby('card6')['isFraud'].value_counts(normalize = True).unstack().plot.bar(stacked = True)\n",
    "b.set_title('Rate of Fraud by Card Type', fontsize = 15)\n",
    "plt.xticks(rotation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Not much difference in fraud rate between credit card and debit card\n",
    "\n",
    "## Examine Email Domain\n",
    "\n",
    "### 1. Purchaser Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.countplot(x = 'P_emaildomain', data = train_full)\n",
    "g.set_title('Purchaser Email Domain Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Email Domain\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks(rotation='vertical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: I see alot of domains came from the same distributors such as hotmail.com, hotmail.fr, yahoo.com, yahoo.fr, yahoo.de, etc. We can group these domains together under the parent distributors.\n",
    "\n",
    "**Action:** Create P_parent_emaildomain field that remove the part after '.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[\"P_parent_emaildomain\"] = train_full[\"P_emaildomain\"].str.split('.', expand = True)[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.countplot(x = 'P_parent_emaildomain', data = train_full)\n",
    "g.set_title('Purchaser Email Domain Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Email Domain\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks(rotation= \"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fewer email domains result in cleaner x tickers. Let's add the fraud rate like in the previous graphs, but this time we add the rate line on top of this graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_emaildomain_fraud_rate = train_full.groupby('P_parent_emaildomain')['isFraud'].value_counts(normalize = True).unstack().fillna(0)[1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.countplot(x = 'P_parent_emaildomain', data = train_full, order = P_emaildomain_fraud_rate.index)\n",
    "g.set_title('Purchaser Email Domain Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Email Domain\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks(rotation= \"vertical\")\n",
    "\n",
    "r = g.twinx()\n",
    "r = sns.pointplot(x = P_emaildomain_fraud_rate.index, y = P_emaildomain_fraud_rate, color = 'blue')\n",
    "r.set_ylabel(\"Fraud Rate\", fontsize = 16, color = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protonmail returns an exemely high fraud rate. Almost 80% of transactions from purchaser using protonmail.com were label fraud. Let's double check this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protonmail_fraud = len(train_full[(train_full['P_parent_emaildomain'] == \"protonmail\") & (train_full['isFraud'] == 1)])\n",
    "protonmail_non_fraud = len(train_full[(train_full['P_parent_emaildomain'] == \"protonmail\") & (train_full['isFraud'] == 0)])\n",
    "\n",
    "protonmail_fraud_rate = protonmail_fraud/ (protonmail_fraud + protonmail_non_fraud)\n",
    "print(\"Number of protonmail fraud transactions:\", protonmail_fraud)\n",
    "print(\"Number of protonmail non-fraud transactions:\", protonmail_non_fraud)\n",
    "print(\"Protonmail fraud rate:\", protonmail_fraud_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Recipient Email\n",
    "\n",
    "Similarly, we can perform the similar analysis on Recepient email domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full[\"R_parent_emaildomain\"] = train_full[\"R_emaildomain\"].str.split('.', expand = True)[[0]]\n",
    "train_full[\"R_parent_emaildomain\"].fillna(\"NA\", inplace=True)\n",
    "\n",
    "R_emaildomain_fraud_rate = train_full.groupby('R_parent_emaildomain')['isFraud'].value_counts(normalize = True).unstack().fillna(0)[1]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.countplot(x = 'R_parent_emaildomain', data = train_full, order = R_emaildomain_fraud_rate.index)\n",
    "g.set_title('Recipient Email Domain Distribution', fontsize = 15)\n",
    "g.set_xlabel(\"Email Domain\", fontsize=15)\n",
    "g.set_ylabel(\"Count\", fontsize=15)\n",
    "plt.xticks(rotation= \"vertical\")\n",
    "\n",
    "r = g.twinx()\n",
    "r = sns.pointplot(x = R_emaildomain_fraud_rate.index, y = R_emaildomain_fraud_rate, color = \"blue\")\n",
    "r.set_ylabel(\"Fraud Rate\", fontsize = 16, color = \"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enjoy this format of visualizing, so I should creat a function that help me explore the categorical format with regard to fraud rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cat_cariable(variable, df=train_full):\n",
    "    train_full[variable].fillna(\"NA\", inplace=True)\n",
    "    variable_fraud_rate = df.groupby(variable)['isFraud'].value_counts(normalize = True).unstack().fillna(0)[1]\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    g = sns.countplot(x = variable, data = df, order = variable_fraud_rate.index)\n",
    "    g.set_title('{} Count'.format(variable), fontsize = 15)\n",
    "    g.set_xlabel(\"{}\".format(variable), fontsize=15)\n",
    "    g.set_ylabel(\"Count\", fontsize=15)\n",
    "    plt.xticks(rotation= \"vertical\")\n",
    "\n",
    "    r = g.twinx()\n",
    "    r = sns.pointplot(x = variable_fraud_rate.index, y = variable_fraud_rate, color = \"blue\")\n",
    "    r.set_ylabel(\"Fraud Rate\", fontsize = 16, color = \"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine M1-M9\n",
    "\n",
    "The transaction data that has comple identity returns mostly NaN except for M4. Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_cat.loc[:,'M1':'M9'].apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('M4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observartion**: Not much variation in fraud rate between M0, M1, and M2 in M4\n",
    "\n",
    "We have gone through all categorical variables in the Transaction Table, now we check out the remaining categorical variables in the Identity Table.\n",
    "\n",
    "## Examine DeviceType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('DeviceType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Fraud rate is higher for mobile device compared to desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine DeviceInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['DeviceInfo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have way too many devices, it makes more sense to select a few devices that has non-trivial count. Let's select categories that have more than 500 counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devicelist = train_full.groupby('DeviceInfo').filter(lambda x: len(x) >500)['DeviceInfo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('DeviceInfo', df = train_full[train_full['DeviceInfo'].isin(devicelist)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: We can see the fraud rate is higher for certain devices\n",
    "\n",
    "## Examine id12 - id38\n",
    "\n",
    "We may generate all the graphs for id12 to id38. Depend on your preference, some graphs may be more informative than the other. The graphs below are selected based on:\n",
    "\n",
    "1. If the graph contains non-masked information (or categories have self-expalainatory meaning)\n",
    "    * For example: 'Found' and 'NotFound' are two categories that by themselves, don't provide us with any helpful information in understanding their relationships with target variable. Perhaps our learner can pickup on the differences, but it's outside of our domain to understand these variables semantically. \n",
    "\n",
    "2. If the graph contains not too many categories so that the xtickers can be plotted legibly\n",
    "\n",
    "You can plots them all out and select for yourself. Here are some of my picks:\n",
    "\n",
    "### IP Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# id_list = train_full.loc[:1, 'id_12':'id_38'].columns\n",
    "\n",
    "# for i in id_list:\n",
    "#     print (visualize_cat_variable(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('id_23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Obervation**: The first notable id plot is the IP status. It is interesting to see the anonymous IP_Proxy would have a higher fraud rate. If someone were to commit a fraudulent transaction, it makes sense that the person would want to protect his/her identity.\n",
    "\n",
    "### Operating Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('id_30')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aggregate the operating system into a few major OSs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['major_os'] = train_full[\"id_30\"].str.split(' ', expand = True)[[0]]\n",
    "\n",
    "visualize_cat_cariable('major_os')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The fraud rate across multiple well-known OSs seem fairly similar. \"Other\" operating systems have a much higher fraud rate.\n",
    "\n",
    "However, it's strange that we see more IOS devices compared to Android, given that Android is the most popular mobile system. If I were to work for Vista, I would ask how the system collects more IOS instances. Could it be that Vista have given us an filtered dataset? Specific market segment? Systematic error or deficiency in collecting Android info?\n",
    "\n",
    "### Browsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_cat_cariable('id_31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as previous plot, we need to reduce the number of categories using aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full['browser'] = train_full[\"id_31\"].str.split(' ', expand = True)[[0]]\n",
    "\n",
    "visualize_cat_cariable('browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few browers that have absurdly high fraud rate. This is likely to due the scarcity of those browsers. We can fix this by apply a minimum-instance-filter. Let's say 0.1 percent of data rows is our cut-off, then each category must have at least 144 instances to be included in our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_list = train_full.groupby('browser').filter(lambda x: len(x) > 144)['browser'].unique()\n",
    "visualize_cat_cariable('browser',  df = train_full[train_full['browser'].isin(browser_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Opera and android browser have relatively high fraud rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explore Numerical Features\n",
    "\n",
    "I anticipate that most variables we will encounter would not follow a normal distribution. Therefore, for each variable, we will explore:\n",
    "\n",
    "1. Distribution\n",
    "\n",
    "2. Log of distribution\n",
    "\n",
    "3. Distribution by target variable\n",
    "\n",
    "4. Log of distribution by target variable\n",
    "\n",
    "5. Boxplot comparison between fraud and non-fraud\n",
    "\n",
    "## Examine Transaction Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_num_variable(variable, df=train_full):\n",
    "    plt.figure(figsize=(12,18))\n",
    "    plt.suptitle('Distribution of: {}'.format(variable), fontsize=22)\n",
    "\n",
    "    plt.subplot(321)\n",
    "    sns.distplot(df[variable], kde= False)\n",
    "    plt.title('{} Distribution'.format(variable), fontsize = 15)\n",
    "\n",
    "    plt.subplot(322)\n",
    "    sns.distplot(np.log10(df[variable]), kde= False)\n",
    "    plt.title('Log-transformed Distribution', fontsize = 15)\n",
    "\n",
    "\n",
    "    plt.subplot(323)\n",
    "    sns.distplot(df[df['isFraud'] == 0][variable], color = 'skyblue', kde= False, label = 'Not Fraud')\n",
    "    sns.distplot(df[df['isFraud'] == 1][variable], color = 'red', kde= False , label = 'Fraud')\n",
    "    plt.title('Fraud vs Non-Fraud Distribution', fontsize = 15)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(324)\n",
    "    sns.distplot(np.log10(df[df['isFraud'] == 0][variable]), color = 'skyblue', kde= False, label = 'Not Fraud')\n",
    "    sns.distplot(np.log10(df[df['isFraud'] == 1][variable]), color = 'red', kde= False , label = 'Fraud')\n",
    "    plt.title('Log-transformed Distribution', fontsize = 15)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(313)\n",
    "    sns.boxplot(x = 'isFraud', y = variable, data = df)\n",
    "    plt.title('Transaction Amount by Fraud', fontsize = 15,  weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_num_variable('TransactionAmt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: \n",
    "    1. TransactionAmt has right-skewed distribution: most transactions are small (less than $200)\n",
    "    2. There is little difference between distribution and average amount for fraud and non-fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Transaction DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_num_variable('TransactionDT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: There is a large number of non-fraud transactions generated at a certain period . This discrepancy also causing the difference in our boxplot.\n",
    "\n",
    "**Possible Improvement**: I should try undersampling the period of non-fraud so that we have less imbalance issue for that particular period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Distance 2\n",
    "\n",
    "Dist1 contains no values. For dist2, we also running into two problems:\n",
    "\n",
    "1. Missing values:\n",
    "\n",
    "    Solution: keeping only the non-null rows in dist2.\n",
    "\n",
    "2. Zero values:\n",
    "\n",
    "    Zero values cause log transform to return infinity values\n",
    "\n",
    "    Solution: add small amount to 0s to avoid infinity\n",
    "    \n",
    "3. Negative values\n",
    "    \n",
    "    The logarithm is only defined for positive numbers. I could perhaps take the log(x+n), where n is the offset values that make the min negative value > 0. However, for such data 0 has a meaning (equality!) that should be respected. Unless I know the meaning of the data, I cannot make arbitrary transformation.\n",
    "    \n",
    "    Solution: no solution, omit the log-transformation graphs\n",
    "\n",
    "Let's update our graphing function with this implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_num_variable(variable, df=train_full.copy()):\n",
    "    # check for homogeneity:\n",
    "    if len(df[variable].unique()) <= 1:\n",
    "        print('{} is a homogeneous set'.format(variable))\n",
    "        return\n",
    "    \n",
    "    # check for NAs and Zeros\n",
    "    if df[variable].isnull().values.any():\n",
    "        df = train_full.dropna(subset=[variable])\n",
    "\n",
    "    if df[variable].min() < 0:\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.suptitle('Distribution of: {}'.format(variable), fontsize=22)\n",
    "    \n",
    "        plt.subplot(221)\n",
    "        sns.distplot(df[variable], kde= False)\n",
    "        plt.title('{} Distribution'.format(variable), fontsize = 15)\n",
    "        \n",
    "        plt.subplot(222)\n",
    "        sns.distplot(df[df['isFraud'] == 0][variable], color = 'skyblue', kde= False, label = 'Not Fraud')\n",
    "        sns.distplot(df[df['isFraud'] == 1][variable], color = 'red', kde= False , label = 'Fraud')\n",
    "        plt.title('Fraud vs Non-Fraud Distribution', fontsize = 15)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(212)\n",
    "        sns.boxplot(x = 'isFraud', y = variable, data = df)\n",
    "        plt.title('{} by Fraud'.format(variable), fontsize = 15,  weight='bold')\n",
    "        \n",
    "    else:\n",
    "        smallest_value = df[df[variable] != 0][variable].min()\n",
    "        if df[variable].min() == 0:\n",
    "            df[variable].replace(0, smallest_value/10, inplace=True)       \n",
    "\n",
    "        plt.figure(figsize=(12,18))\n",
    "        plt.text(x=0.5, y=0.5,\n",
    "                 s=\"Zeros have been replaced with {} to avoid log infinity\".format(smallest_value/10),\n",
    "                 fontsize=12,horizontalalignment='center')\n",
    "\n",
    "        plt.suptitle('Distribution of: {}'.format(variable), fontsize=22)\n",
    "\n",
    "        plt.subplot(321)\n",
    "        sns.distplot(df[variable], kde= False)\n",
    "        plt.title('{} Distribution'.format(variable), fontsize = 15)\n",
    "\n",
    "        plt.subplot(322)\n",
    "        sns.distplot(np.log10(df[variable]), kde= False)\n",
    "        plt.title('Log-transformed Distribution', fontsize = 15)\n",
    "\n",
    "\n",
    "        plt.subplot(323)\n",
    "        sns.distplot(df[df['isFraud'] == 0][variable], color = 'skyblue', kde= False, label = 'Not Fraud')\n",
    "        sns.distplot(df[df['isFraud'] == 1][variable], color = 'red', kde= False , label = 'Fraud')\n",
    "        plt.title('Fraud vs Non-Fraud Distribution', fontsize = 15)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(324)\n",
    "        sns.distplot(np.log10(df[df['isFraud'] == 0][variable]), color = 'skyblue', kde= False, label = 'Not Fraud')\n",
    "        sns.distplot(np.log10(df[df['isFraud'] == 1][variable]), color = 'red', kde= False , label = 'Fraud')\n",
    "        plt.title('Log-transformed Distribution', fontsize = 15)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(313)\n",
    "        sns.boxplot(x = 'isFraud', y = variable, data = df)\n",
    "        plt.title('{} by Fraud'.format(variable), fontsize = 15,  weight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_num_variable('dist2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Dist2 does not seem to varies between fraud and not-fraud\n",
    "\n",
    "## Examine C features\n",
    "\n",
    "Same way of handling a large number of variables, I only choose the notable plots that reflect a large degree of variation. Trying to keep this kernel concised is one of my goals. In this case, I only consider C3 to have some significant patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = train_full.loc[:1, 'C1':'C14'].columns\n",
    "\n",
    "# for i in id_list:\n",
    "#     print (visualize_num_variable(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_num_variable('C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Higher values of C3 associated with no-fraud.\n",
    "\n",
    "C5 and C9 are homogeneous columns.\n",
    "\n",
    "## Examine D features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_list = train_full.loc[:1, 'D1':'D15'].columns\n",
    "\n",
    "# for i in id_list:\n",
    "#     print (visualize_num_variable(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_num_variable('D2')\n",
    "visualize_num_variable('D8')\n",
    "visualize_num_variable('D9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion for EDA:\n",
    "\n",
    "1. Target variable has class imbalance problem where instance of fraud is much lower than non-fraud\n",
    "\n",
    "2. Multiple columns contain too many missing values\n",
    "\n",
    "3. Several columns are homogeneous, therefore, prodvide no useful information in predicting the target variable (this may not be the case for transaction table since we are using a joined table)\n",
    "\n",
    "4. There is period of time where instances of non-fraud far exceed the usual proportion of non-fraud to fraud \n",
    "\n",
    "5. Basic understand of variables can help us do simple feature engineering\n",
    "\n",
    "We will deal with each problem with the purpose of improving the prediction accuracy. But first, let's try a default XGBoost model provided by Vesta. We can use this model as a baseline to compare the improvement (or reduction) of each engineered feature, change, and alteration that  we made along the way.\n",
    "\n",
    "## Brainstorm\n",
    "Before treating this problem like a black box of ensemble learning, it's worthwhile to take our hands off the keyboards and think about the problem of fraud detection in a more \"open-box\" way. There are a lot of intersting questions worth investigating before diving into the madness of hyperparameters tuning. Insights that could lead to trivial and sometimes important questions. Questions that take us on a journey of curiosity and fulfilment. \n",
    "\n",
    "For the data scientists whose minds love to wander. This section is dedicated for silly and serious questions alike.\n",
    "\n",
    "**Scenario** :A Vesta executive storms in the office and excitedly tells everyone that an exciting project has fallen in their laps. It's the fraud detection problem. And he ask his people for some ideas of where to start, which features should be useful in prediciting fraud. He knows it is strange to ask the scientists before attempting any EDA or modeling. After all, they haven't seen a lick of relevant data. But he saids it would be great practive to dip the toe into the water before diving in without any direction. So let's start with the few things that were provided to us: transaction amount, time, card infor, identity, etc... Which information would give us a good start at cracking this problem?\n",
    "\n",
    "Let's define clearly what is a fraud transaction first. \"Fraud detection is a set of activities undertaken to prevent money or property from being obtained through false pretenses\" [Source](https://searchsecurity.techtarget.com/definition/fraud-detection). Most common type of frauds are forging checks or using stolen credit cards. If a person got of hold of your card info, what should he/she do with it? After browsing on Reddit, I found some crude scenarios:\n",
    "\n",
    "1. If you drop your card, it's likely the person who found it by chance and commit a fraud would spend it on consumable and essential products like grocery and gas. The perp will likely go somewhere nearby and spend a larger amount than usual before the card get locked. So perhaps we should look at user's purchase history so that any activities or purchases that deviate from normal buying habit would stand out. But we don't have identifiable data, so we can't go on this route.\n",
    "\n",
    "2. If your information get hacked by careless purchases on some shady websites/gas stations, it's likely that your information will be sold to someone else who use your information for making fraud transaction. This person will make an online purchase and ship it to a distributor, who sells the good for cash and share the profit with the frauder. In this situation, the good is shipped to some far-away place from the user's home address. So the further the distance, the more a transaction looks like a fraud? No, of course not. People sends gifts all the times. But perhaps gifting 3 expensive laptops is slightly more suspicious than gifting a box of chocolate.\n",
    "    \n",
    "    **Feature Engineering:** Combine transaction amount, type of good, and distance together.\n",
    "    \n",
    "3. Fraud commited by someone close to you (family member: spouse, siblings, etc). It's rare, but it could happen.\n",
    "\n",
    "4. Prefered tools for committing fraud. We have learned previously in the EDA that Protonmail has exceptionally high fraud rate >95%. A quick google reveal that Proton is a email service that provide free, anonymous, end-to-end encryption email accounts. Quote from Proton website: \"ProtonMail is incorporated in Switzerland and all our servers are located in Switzerland. This means all user data is protected by strict Swiss privacy laws\". Meaning fraud perpetrator not only protected by the full extend of the privacy law, but also doing it at no cost. Similarly, we have other tools that also have abnormally high fraud rate such as:\n",
    "\n",
    "    * Browser: Comodo IceDragon, Mozilla/Firefox?? (not firefox, but perhaps is Comodo IceDragon but recognized as another version of Firefox?)\n",
    "    \n",
    "    * Operating system: \"other\" category has fraud rate of 60%.\n",
    "    \n",
    "    * Phone (or browser?): Lanix Ilium\n",
    "    \n",
    "    **Feature Engineering:** New features that emphasize the importance of these tools\n",
    "    \n",
    "5. Time of operation. Just like any other jobs, frauders operate at routinely hours that perhaps different from the real users. It is strange, at least to me, to make purchase decision to buy an iphone at 3 in the morning. Again, without historical data, this approach is dead in the egg.\n",
    "\n",
    "\n",
    "# Baseline Model\n",
    "\n",
    "Modeling section is being explore in another private notebook since only 1 GPU instance is allow in Kaggle Kernel..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
